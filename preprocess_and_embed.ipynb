{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'classmethod' object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_google_genai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.1\\Lib\\site-packages\\langchain_huggingface\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     ChatHuggingFace,  \u001b[38;5;66;03m# type: ignore[import-not-found]\u001b[39;00m\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     HuggingFaceEmbeddings,\n\u001b[0;32m      6\u001b[0m     HuggingFaceEndpointEmbeddings,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     HuggingFaceEndpoint,\n\u001b[0;32m     10\u001b[0m     HuggingFacePipeline,\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.1\\Lib\\site-packages\\langchain_huggingface\\chat_models\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# type: ignore[import-not-found]\u001b[39;00m\n\u001b[0;32m      2\u001b[0m     TGI_MESSAGE,\n\u001b[0;32m      3\u001b[0m     TGI_RESPONSE,\n\u001b[0;32m      4\u001b[0m     ChatHuggingFace,\n\u001b[0;32m      5\u001b[0m     _convert_message_to_chat_message,\n\u001b[0;32m      6\u001b[0m     _convert_TGI_message_to_LC_message,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatHuggingFace\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_message_to_chat_message\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTGI_RESPONSE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.1\\Lib\\site-packages\\langchain_huggingface\\chat_models\\huggingface.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_validator\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Self\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface_endpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEndpoint\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFacePipeline\n\u001b[0;32m     41\u001b[0m DEFAULT_SYSTEM_PROMPT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are a helpful, respectful, and honest assistant.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.1\\Lib\\site-packages\\langchain_huggingface\\llms\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface_endpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     HuggingFaceEndpoint,  \u001b[38;5;66;03m# type: ignore[import-not-found]\u001b[39;00m\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFacePipeline\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFaceEndpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFacePipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.1\\Lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:27\u001b[0m\n\u001b[0;32m     17\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m VALID_TASKS \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext2text-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversational\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m )\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mHuggingFaceEndpoint\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mLLM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;43;03m    HuggingFace Endpoint.\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m \n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_url\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.1\\Lib\\site-packages\\pydantic\\v1\\main.py:221\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[1;34m(mcs, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_valid_field(var_name) \u001b[38;5;129;01mand\u001b[39;00m var_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m annotations \u001b[38;5;129;01mand\u001b[39;00m can_be_changed:\n\u001b[0;32m    220\u001b[0m     validate_field_name(bases, var_name)\n\u001b[1;32m--> 221\u001b[0m     inferred \u001b[38;5;241m=\u001b[39m \u001b[43mModelField\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUndefined\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_validators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m var_name \u001b[38;5;129;01min\u001b[39;00m fields:\n\u001b[0;32m    229\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lenient_issubclass(inferred\u001b[38;5;241m.\u001b[39mtype_, fields[var_name]\u001b[38;5;241m.\u001b[39mtype_):\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.1\\Lib\\site-packages\\pydantic\\v1\\fields.py:504\u001b[0m, in \u001b[0;36mModelField.infer\u001b[1;34m(cls, name, value, annotation, class_validators, config)\u001b[0m\n\u001b[0;32m    501\u001b[0m     required \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    502\u001b[0m annotation \u001b[38;5;241m=\u001b[39m get_annotation_from_field_info(annotation, field_info, name, config\u001b[38;5;241m.\u001b[39mvalidate_assignment)\n\u001b[1;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43malias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_validators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequired\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequired\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.1\\Lib\\site-packages\\pydantic\\v1\\fields.py:434\u001b[0m, in \u001b[0;36mModelField.__init__\u001b[1;34m(self, name, type_, class_validators, model_config, default, default_factory, required, final, alias, field_info)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m SHAPE_SINGLETON\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mprepare_field(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 434\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.1\\Lib\\site-packages\\pydantic\\v1\\fields.py:544\u001b[0m, in \u001b[0;36mModelField.prepare\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    538\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;124;03m    Prepare the field but inspecting self.default, self.type_ etc.\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m    Note: this method is **not** idempotent (because _type_analysis is not idempotent),\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03m    e.g. calling it it multiple times may modify the field and configure it incorrectly.\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_default_and_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m ForwardRef \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m DeferredType:\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;66;03m# self.type_ is currently a ForwardRef and there's nothing we can do now,\u001b[39;00m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;66;03m# user will need to call model.update_forward_refs()\u001b[39;00m\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.1\\Lib\\site-packages\\pydantic\\v1\\fields.py:568\u001b[0m, in \u001b[0;36mModelField._set_default_and_type\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m errors_\u001b[38;5;241m.\u001b[39mConfigError(\n\u001b[0;32m    564\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myou need to set the type of field \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m when using `default_factory`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    565\u001b[0m         )\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 568\u001b[0m default_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m default_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_ \u001b[38;5;129;01mis\u001b[39;00m Undefined:\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_ \u001b[38;5;241m=\u001b[39m default_value\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.1\\Lib\\site-packages\\pydantic\\v1\\fields.py:437\u001b[0m, in \u001b[0;36mModelField.get_default\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_default\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msmart_deepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_factory()\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.1\\Lib\\site-packages\\pydantic\\v1\\utils.py:694\u001b[0m, in \u001b[0;36msmart_deepcopy\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m):\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;66;03m# do we really dare to catch ALL errors? Seems a bit risky\u001b[39;00m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.1\\Lib\\copy.py:162\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    160\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    161\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.1\\Lib\\copy.py:259\u001b[0m, in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 259\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    261\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.1\\Lib\\copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.1\\Lib\\copy.py:221\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    219\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 221\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.1\\Lib\\copy.py:151\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    149\u001b[0m reductor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce_ex__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reductor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     rv \u001b[38;5;241m=\u001b[39m \u001b[43mreductor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot pickle 'classmethod' object"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import glob\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I am doing well, thank you for asking! How are you today?\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-0b406eef-a915-4a84-aa68-061e41267b57-0', usage_metadata={'input_tokens': 7, 'output_tokens': 16, 'total_tokens': 23, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(google_api_key=google_api_key, model=\"gemini-1.5-pro-latest\")\n",
    "llm.invoke(\"Hello, how are you?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name = \"BAAI/bge-m3\"\n",
    ")\n",
    "persist_directory = \"persisted_embeddings_2\"\n",
    "\n",
    "# pdf_files = glob.glob(\"./Data/*.pdf\")\n",
    "# pages = []\n",
    "\n",
    "# for pdf_file in pdf_files:\n",
    "#     pages.extend(PyPDFLoader(pdf_file).load_and_split())\n",
    "\n",
    "# doc_chunks = []\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=850,\n",
    "#     separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "#     chunk_overlap=100,\n",
    "# )\n",
    "# chunks = text_splitter.split_documents(pages)\n",
    "# len(chunks)\n",
    "\n",
    "\n",
    "\n",
    "# vectordb = Chroma.from_documents(\n",
    "#     documents=chunks,\n",
    "#     embedding=embedder,\n",
    "#     persist_directory=persist_directory\n",
    "# ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471\n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma(persist_directory=persist_directory,embedding_function=embedder)\n",
    "retriever = vectordb.as_retriever()\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reply(user_query) : \n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = retriever.invoke(user_query)\n",
    "    \n",
    "    # Format the retrieved documents with content \n",
    "    retrieved_context = \"\\n\".join([f\"{doc.page_content} \\n\" for doc in relevant_docs])\n",
    "    \n",
    "    # Create the prompt for the LLM\n",
    "    llm_prompt = f\"\"\"\n",
    "    You are a helpful assistant specialized in answering queries based on specific documents.\n",
    "    Your role is to provide accurate, detailed answers that show how you used the context to answer.\n",
    "    Always base your response on the information retrieved.\n",
    "    \n",
    "    Context:\n",
    "    {retrieved_context}\n",
    "    \n",
    "    Question: {user_query}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the response from the language model\n",
    "    response = llm.invoke(llm_prompt).content\n",
    "    # Extract and format document names\n",
    "    unique_document_names = {doc.metadata.get('source', 'Unknown') for doc in relevant_docs}\n",
    "    document_references = \"\\n\".join(f\"- {name}\"[9:] for name in unique_document_names)\n",
    "\n",
    "\n",
    "    # Combine the LLM response with document references\n",
    "    final_response = f\"{response}\\n\\n**References:**\\n{document_references}\"\n",
    "    print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CEO of GitLab is Sid Sijbrandij. He is also the co-founder and board chair of the company. He believes in transparency and iteration, and that negative feedback is important for improvement. He also believes that values are not binary and that there is always room for interpretation.  He emphasizes that the results matter most and that transparency should not be pursued for its own sake. \n",
      "\n",
      "\n",
      "**References:**\n",
      "GitLab Values _ The GitLab Handbook.pdf\n",
      "CEO _ The GitLab Handbook.pdf\n"
     ]
    }
   ],
   "source": [
    "reply(\"What do you know about the CEO of Gitlab?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "load_dotenv()\n",
    "path = \"./figures/\"\n",
    "\n",
    "# Get elements\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=\"./Data/About the Handbook _ The GitLab Handbook.pdf\",\n",
    "    # Using pdf format to find embedded image blocks\n",
    "    extract_images_in_pdf=True,\n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "    # Post processing to aggregate text once we have the title\n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Attempt to create a new chunk 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars\n",
    "    # Hard max on chunks\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.CompositeElement at 0x2184f1a74d0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x2184f1a5c40>,\n",
       " <unstructured.documents.elements.Table at 0x2184f1a6de0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x2184f1a7530>,\n",
       " <unstructured.documents.elements.Table at 0x2184f1284a0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x2184f0eede0>,\n",
       " <unstructured.documents.elements.Table at 0x2184edb4110>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x2184edb5a60>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x2184f1a7e60>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pdf_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending texts and tables from the pdf file\n",
    "def data_category(raw_pdf_elements): # we may use decorator here\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "           tables.append(str(element))\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "           texts.append(str(element))\n",
    "    data_category = [texts,tables]\n",
    "    return data_category\n",
    "texts = data_category(raw_pdf_elements)[0]\n",
    "tables = data_category(raw_pdf_elements)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['22/10/2024, 14:54\\n\\nAbout the Handbook | The GitLab Handbook\\n\\nThe GitLab Handbook\\n\\nGitLab TeamOps Handbook Job Families Reports\\n\\nAbout the Handbook\\n\\nHistory of the handbook\\n\\nThe handbook started when GitLab was a company of just ten people to make sharing information efficient and easy. We knew that future GitLab team-members wouldnʼt be able to see emails about process changes that were being sent before they joined and that most of the people who would eventually join GitLab likely hadnʼt even heard of us yet. The handbook was our way of ensuring that all of our company information was accessible to everyone regardless of when they became part of the team.\\n\\nAdvantages\\n\\nAt GitLab our handbook is extensive and keeping it relevant is an important part of\\n\\neveryoneʼs job. It is a vital part of who we are and how we communicate. We established these processes because we saw these benefits:\\n\\n. Reading is much faster than listening.\\n\\n. Reading is async, you donʼt have to interrupt someone or wait for them to become\\n\\navailable.\\n\\n. Talent Acquisition is easier if people can see what we stand for and how we operate.\\n\\n. Retention is better if people know what they are getting into before they join. . On-boarding is easier if you can find all relevant information spelled out. . Teamwork is easier if you can read how other parts of the company work. . Discussing changes is easier if you can read what the current process is. . Communicating change is easier if you can just point to the diff.\\n\\n. Everyone can contribute to it by proposing a change via a merge request.\\n\\nOo . Everyone can contribute to it by proposing a change via a merge request.\\n\\nOne common concern newcomers to the handbook express is that the strict\\n\\ndocumentation makes the company more rigid. In fact, writing down our current process in the handbook has the effect of empowering contributors to propose change. As a result, this handbook is far from rigid. You only need to look at the handbook changelog to see the evidence. Every attempt is made to document guidelines and processes in the handbook. However, it is not possible to document every possible situation or scenario that could potentially occur. Just because something is not yet in the handbook does not mean that it\\n\\nhttps://handbook.gitlab.com/handbook/about/\\n\\n1/6\\n\\n22/10/2024, 14:54\\n\\nAbout the Handbook | The GitLab Handbook\\n\\nis allowed. GitLab will review each team memberʼs concern or situation based on local laws to determine the best outcome and then update the handbook accordingly. If you have questions, please discuss with your manager or contact the People Success team.',\n",
       " 'Handbook Interpretation\\n\\nThe handbook is subject to interpretation. We do our best to be as clear as possible to minimize confusion and/or misinterpretation. We also recognize that we have a global audience and that may bring different interpretations. If you have any questions or need further clarification please check with the content owner of the page. When in doubt please reach out and ask.\\n\\nRemember that everything is in draft at GitLab and subject to change, this includes our handbook.\\n\\nCount handbook pages\\n\\nItʼs easy to see that the handbook is large, but have you ever wondered just how large? The handbook is over two thousand pages long. Thatʼs a lot of good info!\\n\\nHistorical Word and Page Counts\\n\\nabout.gitlab.com/handbook\\n\\nDate\\n\\nWord Count Page Count\\n\\nNotes',\n",
       " 'https://handbook.gitlab.com/handbook/about/\\n\\n2/6\\n\\n22/10/2024, 14:54\\n\\nAbout the Handbook | The GitLab Handbook\\n\\nDate\\n\\nWord Count Page Count\\n\\nNotes',\n",
       " 'handbook.gitlab.com/handbook\\n\\nDate\\n\\nWord Count Page Count\\n\\nNotes',\n",
       " 'Methodology\\n\\nWord and page counts are determined through a simple two-step process:\\n\\n. Count the number of words in the handbook. This can be done by running find sites/handbook/source/handbook -type f -name \"*.md\" -o -name \"*.md.erb\" | xargs wc -w from the root of the repository.\\n\\nhttps://handbook.gitlab.com/handbook/about/\\n\\n3/6\\n\\n22/10/2024, 14:54\\n\\nAbout the Handbook | The GitLab Handbook\\n\\n. Count the number of pages in the handbook. This can be done by running grep -l -r \"\\\\\" * | wc -l from the root of the repository.\\n\\nNote: If you need to go back to an earlier version of the handbook, use git checkout `git rev-list -n 1 --first-parent --before=\"2021-07-02 00:00\" master` specifying the next day after the day you want.\\n\\nView Statistics\\n\\nOn this page you can see handbook trends and discover popular pages that you may not know about.\\n\\nGitLab uses Snowplow to track handbook usage; the information can be viewed on Tableau.\\n\\nMore about the handbook\\n\\nWeʼve gathered some information about the handbook here, but thereʼs still more elsewhere.\\n\\nHandbook usage\\n\\nEvolution of the handbook on the CEO page\\n\\nChangelog\\n\\nHandbook editing examples\\n\\nContent Websites\\n\\nOverview\\n\\nThis area has traditionally been referred to as “the handbook”, but over time has grown in scope to include multiple sites, projects, repos, and types of content.\\n\\nTherefore, we are using the term “content websites” here to avoid ambiguity and properly frame discussions around this scope of responsibility.\\n\\nSee our direction page for more.\\n\\nIf you need help, please see the editing handbook section, or escalation information if itʼs urgent.\\n\\nEditing the Handbook\\n\\nhttps://handbook.gitlab.com/handbook/about/\\n\\n4/6\\n\\n22/10/2024, 14:54\\n\\nAbout the Handbook | The GitLab Handbook\\n\\nInformation on how to edit the handbook including tips and troubleshooting.\\n\\nGitLab Handbook Usage\\n\\nAbout how GitLab uses the handbook.\\n\\nHandbook Changelog\\n\\nThe last 100 Merge Requests to the Handbook\\n\\nHandbook Direction\\n\\nOverview\\n\\nThe GitLab Handbook is the single source of truth for how we operate at GitLab, including processes, policies, and product direction. In keeping with our value of transparency, the GitLab Handbook is entirely open to the world. We welcome feedback from the community and hope that it serves as inspiration for other current or future companies. The GitLab Handbook is also an incredible talent acquisition tool, providing candidates with valuable insight into how GitLab runs as a company.',\n",
       " 'Handbook Escalation\\n\\nFor information on team membersʼ roles and responsibilities, see Content Websites page.\\n\\nIntroduction\\n\\nThe Handbook is a critical part of empowering team members to do their jobs effectively. As such, we have a group of team members who assist in resolving issues affecting all team members.\\n\\nReporting an issue\\n\\nIf youʼre looking for general help, please see the editing handbook page.\\n\\nAny issues should be reported in the #handbook-escalation channel in Slack.\\n\\nHandbook Style Guide\\n\\nGitLabʼs general communications practices are detailed at GitLab Communication, but beyond those we do have some channel-specific guidance available. See the list of resources below for links to other guides.\\n\\nHandbook style guidance is covered primarily by the handbook markdown guide, and the editing handbook page.\\n\\nIn the absence of handbook-specific guidance, follow:\\n\\nhttps://handbook.gitlab.com/handbook/about/\\n\\n5/6\\n\\n22/10/2024, 14:54\\n\\nAbout the Handbook | The GitLab Handbook\\n\\n. GitLabʼs Writing Style Guidelines, or\\n\\n. the Documentation Style Guide.\\n\\nRelated Resources\\n\\nGitLab Communication\\n\\nMarkdown guide\\n\\nGitLab Documentation guidelines\\n\\nDocumentation style Guide\\n\\nGitLab style guides\\n\\nPajamas Design System\\n\\nMarketing site content style guide\\n\\nBlog style guide\\n\\nLast modified September 23, 2024: Fix broken links (d748cf8c)\\n\\nView page source - Edit this page - please contribute.\\n\\nhttps://handbook.gitlab.com/handbook/about/\\n\\n6/6']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2018-01-01 298,806 228 2018-10-01 427,929 335 2019-01-01 520,519 439 2019-04-01 656,668 586 2019-07-01 818,064 766 2019-10-01 987,397 884 2020-01-01 1,204,642 1,035 2020-04-01 1,491,017 1,222 2020-07-01 1,851,350 1,488',\n",
       " '2020-10-01 2,166,627 1,759 2021-01-01 2,410,554 1,914 2021-04-01 2,615,372 2,056 2021-07-01 2,956,781 2,271 2021-10-01 3,138,952 2,355 2022-01-01 3,280,108 2,395 2022-04-01 3,474,993 2,553 2022-07-01 3,628,280 2,641 2022-10-01 3,732,384 2,724 2023-01-01 3,732,186 2,722 ⬅ Last entry before migration 2023-07-01 3,905,979 2,743 2023-10-01 3,478,407 2,306 2023-12-22 0 0 ⬅ Handbook Migration Complete',\n",
       " '2023-07-01 150,732 133 2023-10-01 1,909,139 1,403 2024-01-01 3,631,360 3,003 ⬅ Migration Completed 2024-10-22 3,855,949 3,086 ⬅ Hugo generated Live Count']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "def tables_summarize(data_category):\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables. \\\n",
    "                    Give a concise summary of the table. Table chunk: {element} \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | llm | StrOutputParser()\n",
    "    table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "    \n",
    "\n",
    "    return table_summaries\n",
    "table_summaries = tables_summarize(data_category)\n",
    "text_summaries = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The table shows consistent growth in two numerical variables between January 2018 and July 2020.  The first variable increases from approximately 300,000 to 1,850,000 and the second from 228 to 1,488 over this period.\\n',\n",
       " 'The table shows a general upward trend in both columns (likely representing some kind of volume and count) from October 2020 to July 2023, followed by a drop in both columns in October 2023 and finally reaching zero in December 2023, marked as \"Handbook Migration Complete\".  This suggests the migration caused a reset or transfer of the tracked quantities.\\n',\n",
       " 'Migration was completed on 2024-01-01 with 3,631,360 items and 3,003 users.  A later live count on 2024-10-22 showed 3,855,949 items and 3,086 users.\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "\n",
    "def encode_image(image_path):\n",
    "    ''' Getting the base64 string '''\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def image_captioning(img_base64,prompt):\n",
    "    ''' Image summary '''\n",
    "\n",
    "    msg = llm.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\":prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{img_base64}\"\n",
    "                        },\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Store base64 encoded images\n",
    "img_base64_list = []\n",
    "\n",
    "# Store image summaries\n",
    "image_summaries = []\n",
    "\n",
    "# Prompt : Our prompt here is customized to the type of images we have which is chart in our case\n",
    "prompt = \"Describe the image in detail. Be specific about graphs, such as bar plots.\"\n",
    "\n",
    "# Read images, encode to base64 strings\n",
    "for img_file in sorted(os.listdir(path)):\n",
    "    if img_file.endswith('.jpg'):\n",
    "        img_path = os.path.join(path, img_file)\n",
    "        base64_image = encode_image(img_path)\n",
    "        img_base64_list.append(base64_image)\n",
    "        img_capt = image_captioning(base64_image,prompt)\n",
    "        # time.sleep(60)\n",
    "        image_summaries.append(img_capt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The image is a black rectangular box with a thin white border. Inside the box, in white text, is \"(CC) BY-SA\". The text is left-aligned.  There are no graphs or other elements besides the text and the border.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_capt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64decode \n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    ''' Split base64-encoded images and texts '''\n",
    "    b64 = []\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            b64decode(doc)\n",
    "            b64.append(doc)\n",
    "        except Exception as e:\n",
    "            text.append(doc)\n",
    "    return {\n",
    "        \"images\": b64,\n",
    "        \"texts\": texts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "import uuid\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name = \"BAAI/bge-m3\"\n",
    ")\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"multi_modal_rag\",\n",
    "                     embedding_function=embedder)\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(text_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "# Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "    for i, s in enumerate(table_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "\n",
    "# Add image summaries\n",
    "img_ids = [str(uuid.uuid4()) for _ in img_base64_list]\n",
    "summary_img = [\n",
    "    Document(page_content=s, metadata={id_key: img_ids[i]})\n",
    "    for i, s in enumerate(image_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_img)\n",
    "retriever.docstore.mset(list(zip(img_ids, img_base64_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "\n",
    "def prompt_func(dict):\n",
    "    format_texts = \"\\n\".join(dict[\"context\"].get(\"texts\", []))\n",
    "\n",
    "    # Construct the message content with text and tables\n",
    "    content = [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"\"\"Answer the question based on the following context, which can include text, tables, and optionally an image : make your \n",
    "            answer as detailed as possible and provide reasoning for your answer : \n",
    "Question: {dict[\"question\"]}\n",
    "\n",
    "Text and tables:\n",
    "{format_texts}\n",
    "\"\"\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Add the image only if it exists\n",
    "    if dict[\"context\"].get(\"images\"):\n",
    "        content.append(\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{dict['context']['images'][0]}\"\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Return the formatted HumanMessage\n",
    "    return [HumanMessage(content=content)]\n",
    "\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(prompt_func)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The GitLab handbook is stated to be \"over two thousand pages long\".  While specific historical word and page counts are referenced as being available, the provided text doesn\\'t display any of those historical values. It does describe the methods used to calculate these metrics:\\n\\n* **Page Count:** `grep -l -r \"\\\\\" * | wc -l` run from the root of the repository.  This command essentially counts the number of files containing a backslash, which seems like an unusual way to count pages. It\\'s possible this is a typo and a different character was intended.\\n* **Word Count:** `find sites/handbook/source/handbook -type f -name \"*.md\" -o -name \"*.md.erb\" | xargs wc -w` run from the root of the repository. This command finds all `.md` and `.md.erb` files within the specified directory and counts the words in them.\\n\\nThe text also mentions using Snowplow and Tableau for tracking handbook usage and viewing statistics, and links to a few potentially relevant pages (which are inaccessible within this context).\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"tell me about handbook pages count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
